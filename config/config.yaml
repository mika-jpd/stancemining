device_map: 'auto'
hf_token: <replace-with-your-token>

data:
  dataset: semeval

model:
  llmmodelname: meta-llama/Llama-3.2-1B-Instruct
  method: pacte
  annotatorname: None
  stancemethod: llm
  mintopicsize: 3
  llm_method: finetuned
  target_type: noun_phrase

test:
  in_context_prompt: False
  finetuned_model: False
  enable_thinking: False
  sample_dataset: None

finetune:
  task: stance-classification
  model_name: meta-llama/Llama-3.2-1B-Instruct
  quantization: None
  add_system_message: True
  do_train: True
  do_eval: True
  num_epochs: 2
  grad_accum_steps: 8
  learning_rate: 1e-4
  save_model_path: ./models/stancemining/
  eval_steps: 10
  prompting_method: stancemining
  classification_method: generation
  generation_method: list
  batch_size: 1
  attn_implementation: flash_attention_2
  continue_training: False
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1

wiba:
  task: stance-classification
  model_name: meta-llama/Llama-3.2-1B-Instruct
  add_system_message: True
  do_train: True
  do_eval: True
  num_epochs: 2
  save_model_path: ./models/wiba/
  eval_steps: 100
  prompting_method: wiba
  classification_method: head
  generation_method: beam
  batch_size: 1
  quantization: None
  grad_accum_steps: 8
  learning_rate: 1e-4
